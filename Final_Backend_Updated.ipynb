# ‚úÖ Install required packages
!pip install -q google-generativeai langchain chromadb sentence-transformers pyngrok

# 1Ô∏è‚É£ Imports & Config
from google.colab import files as colab_files
import google.generativeai as genai
import zipfile, os, shutil, json, re
from pyngrok import ngrok
import threading
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from datetime import datetime
import uuid
import http.server
import socketserver
import threading
from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings
from langchain.vectorstores import Chroma, FAISS
import numpy as np


# Configure Gemini API key (using your provided key)
genai.configure(api_key="AIzaSyDDQfV8AolWWSIM6YatI-KVHpW9UaojXC0")

# Configure ngrok auth token (using your provided token)
ngrok.set_auth_token("2vyG8rxMvWumV57l34zFYbJgXU7_5Eq6ntAuoqZvffhTQ9M2J")

# Expanded supported extensions
EXT_DIRS = [
    # Web & Frontend
    'html','css','js','jsx','ts','tsx','vue','svelte','json','xml',
    # Backend & Server
    'py','php','rb','java','go','rs','cs','c','cpp',
    # Config & Build
    'yaml','yml','toml','ini','conf','env','lock','json','md','gitignore',
    # Scripts & Others
    'sh','bat','sql','pl','r','scala','bash','dockerfile'
]

# Frontend framework detection patterns
FRAMEWORK_PATTERNS = {
    'react': [
        'import React', 'React.Component', 'useState', 'useEffect',
        'ReactDOM', 'createRoot', 'jsx', 'tsx'
    ],
    'angular': [
        '@Component', '@NgModule', '@Injectable', 'ngOnInit',
        'ngAfterViewInit', 'ngFor', 'ngIf'
    ],
    'vue': [
        'Vue.createApp', 'defineComponent', 'setup()', '<template>',
        '@vue/cli', 'Composition API', 'Options API'
    ],
    'nodejs': [
        'require(', 'module.exports', 'express()', 'npm', 'package.json',
        'node_modules', 'process.env'
    ]
}

# Initialize RAG components
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
vectorstore = None
chat_history = []
# Add this after the FRAMEWORK_PATTERNS dictionary
class VectorEmbeddingManager:
    def _init_(self, embedding_model="all-MiniLM-L6-v2", embedding_type="huggingface",
                 persist_directory="./chroma_db", openai_api_key=None):
        self.embedding_type = embedding_type
        self.persist_directory = persist_directory

        # Initialize embeddings based on type
        if embedding_type == "huggingface":
            self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
            self.dimension = 384  # Default for all-MiniLM-L6-v2
        elif embedding_type == "openai":
            if not openai_api_key:
                raise ValueError("OpenAI API key is required for OpenAI embeddings")
            os.environ["OPENAI_API_KEY"] = openai_api_key
            self.embeddings = OpenAIEmbeddings()
            self.dimension = 1536  # Default for OpenAI embeddings
        else:
            raise ValueError(f"Unsupported embedding type: {embedding_type}")

        self.vectorstore = None

    def create_vectorstore(self, documents, vectorstore_type="chroma"):
        """Create a vector store from documents"""
        if vectorstore_type == "chroma":
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
        elif vectorstore_type == "faiss":
            self.vectorstore = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings
            )
        else:
            raise ValueError(f"Unsupported vectorstore type: {vectorstore_type}")

        return self.vectorstore

    def add_documents(self, documents):
        """Add documents to existing vectorstore"""
        if not self.vectorstore:
            raise ValueError("Vectorstore not initialized. Call create_vectorstore first.")

        self.vectorstore.add_documents(documents)
        return self.vectorstore

    def similarity_search(self, query, k=5):
        """Perform similarity search"""
        if not self.vectorstore:
            raise ValueError("Vectorstore not initialized")

        return self.vectorstore.similarity_search_with_score(query, k=k)

    def get_document_embedding(self, text):
        """Get embedding for a single text"""
        return self.embeddings.embed_query(text)

    def compute_similarity(self, text1, text2):
        """Compute cosine similarity between two texts"""
        embedding1 = self.get_document_embedding(text1)
        embedding2 = self.get_document_embedding(text2)

        # Compute cosine similarity
        similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
        return similarity

# 2Ô∏è‚É£ Upload & extract your ZIP
def upload_and_extract():
    uploaded = colab_files.upload()
    orig_zip = next(iter(uploaded))
    print(f"üì¶ Uploaded: {orig_zip}")

    extract_root = "uploads"
    if os.path.exists(extract_root): shutil.rmtree(extract_root)
    os.makedirs(extract_root, exist_ok=True)
    with zipfile.ZipFile(orig_zip, 'r') as z: z.extractall(extract_root)
    print("‚úÖ ZIP extracted to /content/uploads")
    return orig_zip

# 3Ô∏è‚É£ Detect Project Structure and Config Files
def analyze_project_structure(extract_root):
    project_aim = None
    project_type = "unknown"
    config_files = {}
    framework_scores = {framework: 0 for framework in FRAMEWORK_PATTERNS.keys()}

    # Look for README and config files
    for root, _, files in os.walk(extract_root):
        for file in files:
            file_lower = file.lower()
            filepath = os.path.join(root, file)

            # Find README
            if file_lower == "readme.md":
                with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                    project_aim = f.read()
                print("üß† README.md found ‚Äî using it as the project aim.")

            # Collect config files
            if file_lower in ["package.json", "angular.json", "tsconfig.json",
                             "webpack.config.js", ".babelrc", "vite.config.js",
                             "nuxt.config.js", "next.config.js", "svelte.config.js"]:
                try:
                    with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                        config_files[file_lower] = f.read()
                    print(f"üìÑ Found config file: {file_lower}")
                except:
                    print(f"‚ö† Error reading config file: {file_lower}")

            # Analyze file content to detect frameworks
            try:
                _, ext = os.path.splitext(file_lower)
                if ext[1:] in ['js', 'jsx', 'ts', 'tsx', 'html', 'vue', 'svelte']:
                    with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                        content = f.read()
                        for framework, patterns in FRAMEWORK_PATTERNS.items():
                            for pattern in patterns:
                                if pattern in content:
                                    framework_scores[framework] += 1
            except:
                pass

    # Determine project type from scores and config files
    if "package.json" in config_files:
        project_type = "nodejs"
        try:
            package_json = json.loads(config_files["package.json"])
            dependencies = {**package_json.get("dependencies", {}), **package_json.get("devDependencies", {})}

            if "react" in dependencies or "react-dom" in dependencies:
                project_type = "react"
                if "next" in dependencies:
                    project_type = "react-nextjs"
            elif "vue" in dependencies:
                project_type = "vue"
                if "nuxt" in dependencies:
                    project_type = "vue-nuxt"
            elif "@angular/core" in dependencies:
                project_type = "angular"
            elif "svelte" in dependencies:
                project_type = "svelte"
        except:
            pass

    # If config detection didn't work, use pattern matching results
    if project_type == "nodejs":
        max_framework = max(framework_scores.items(), key=lambda x: x[1])
        if max_framework[1] > 0:
            project_type = max_framework[0]

    print(f"üîç Detected project type: {project_type}")
    return project_aim, project_type, config_files

# 4Ô∏è‚É£ Segregate code files with improved classification
def segregate_files(extract_root):
    segregated = {ext: [] for ext in EXT_DIRS}
    special_dirs = {
        "components": [],
        "pages": [],
        "hooks": [],
        "services": [],
        "utils": [],
        "assets": [],
        "styles": [],
        "api": [],
        "tests": []
    }

    for root, dirs, fnames in os.walk(extract_root):
        # Skip node_modules and other large dependency directories
        if any(excluded in root for excluded in ["node_modules", "dist", "build", ".git"]):
            continue

        # Check if we're in a special directory
        rel_path = os.path.relpath(root, extract_root)
        current_dir = os.path.basename(root).lower()
        special_dir_match = None
        for special_dir in special_dirs.keys():
            if special_dir in current_dir or special_dir in rel_path.lower():
                special_dir_match = special_dir
                break

        for fn in fnames:
            if '.' not in fn: continue
            ext = fn.rsplit('.',1)[1].lower()
            if ext in EXT_DIRS:
                tgt = os.path.join("code_dump", ext)
                os.makedirs(tgt, exist_ok=True)
                src = os.path.join(root, fn)
                dst = os.path.join(tgt, fn)
                shutil.copy(src, dst)
                segregated[ext].append(dst)

                # Also add to special directory if applicable
                if special_dir_match:
                    tgt_special = os.path.join("code_dump_organized", special_dir_match)
                    os.makedirs(tgt_special, exist_ok=True)
                    dst_special = os.path.join(tgt_special, fn)
                    shutil.copy(src, dst_special)
                    special_dirs[special_dir_match].append(dst_special)

    print("‚úÖ Files segregated into code_dump/ and code_dump_organized/")
    return segregated, special_dirs

# 5Ô∏è‚É£ Create RAG knowledge base from code
def build_rag_knowledge_base(segregated, project_aim, config_files, embedding_model="all-MiniLM-L6-v2"):
    global vectorstore

    # Initialize the embedding manager
    embedding_manager = VectorEmbeddingManager(
        embedding_model=embedding_model,
        embedding_type="huggingface",
        persist_directory="./chroma_db"
    )

    # Prepare documents for RAG
    documents = []

    # Add project aim
    if project_aim:
        documents.append({
            "content": project_aim,
            "metadata": {"source": "README.md", "type": "documentation"}
        })

    # Add config files
    for config_name, content in config_files.items():
        documents.append({
            "content": content,
            "metadata": {"source": config_name, "type": "configuration"}
        })

    # Add code files
    for ext, paths in segregated.items():
        for path in paths:
            try:
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    filename = os.path.basename(path)
                    documents.append({
                        "content": content,
                        "metadata": {"source": filename, "language": ext, "type": "code"}
                    })
            except:
                print(f"‚ö† Error reading file: {path}")

    # Create text chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = []
    for doc in documents:
        doc_chunks = text_splitter.create_documents(
            texts=[doc["content"]],
            metadatas=[doc["metadata"]]
        )
        chunks.extend(doc_chunks)

    # Create vector store
    vectorstore = embedding_manager.create_vectorstore(chunks)

    print(f"‚úÖ Created RAG knowledge base with {len(chunks)} chunks from {len(documents)} documents")
    return vectorstore, embedding_manager

# 6Ô∏è‚É£ Gemini Analysis Function with Framework-Specific Prompts
def analyze_code(code_input, fname, project_type, aim=None, similar_files=None):
    model = genai.GenerativeModel(model_name="gemini-2.0-flash")

    # Create framework-specific guidance based on detected project type
    framework_guidance = ""
    if project_type == "react":
        framework_guidance = """
- Check for React best practices (hooks rules, useCallback/useMemo optimization)
- Ensure components follow Single Responsibility Principle
- Verify proper state management (useState, useReducer, Context, or external state)
- Check for correct key usage in lists
- Ensure proper effect cleanup to prevent memory leaks
- Assess component re-rendering optimization
"""
    elif project_type == "angular":
        framework_guidance = """
- Check for proper component lifecycle management
- Verify correct usage of services and dependency injection
- Assess template syntax and binding correctness
- Evaluate usage of Angular directives and pipes
- Check for proper module organization
- Verify correct use of observables and subscription management
"""
    elif project_type == "vue":
        framework_guidance = """
- Verify proper component lifecycle hook usage
- Check correct usage of computed properties vs methods
- Evaluate props validation and component communication
- Assess reactivity edge cases and limitations
- Verify proper usage of Vue-specific directives
- Check correct store pattern implementation (Vuex/Pinia)
"""
    elif project_type == "nodejs":
        framework_guidance = """
- Check for proper async/await and Promise handling
- Verify error handling and middleware patterns
- Assess database connection management
- Check for security best practices (input validation, auth)
- Evaluate API design and RESTful principles
- Verify environment variable usage and configuration
"""

    # Get similar files context if available
    similar_context = ""
    if similar_files:
        similar_context = "Similar files in the project that might provide context:\n"
        for i, (file, score) in enumerate(similar_files[:3]):
            similar_context += f"{i+1}. {file.metadata['source']} (similarity: {score:.2f})\n"

    if aim:
        prompt = f"""
You are a professional software architect specifically experienced with {project_type} development.
The project goal is:

üìò *Project Aim (from README.md)*:
{aim.strip()}

{similar_context}

Now analyze the following file {fname} and refactor it while preserving project intent:


{code_input}


Provide an in-depth review and updated version of the file focused on {project_type} best practices:

1. Framework-Specific Issues
{framework_guidance}

2. Logical Errors & Edge Cases
- Analyze conditional logic for correctness and completeness
- Identify off-by-one errors and boundary condition handling
- Verify business logic implementation
- Check for race conditions in async code

3. Performance & Scalability
- Optimize rendering performance (for UI components)
- Improve data fetching and caching strategies
- Enhance bundle size optimization techniques
- Identify memory leaks from closures or reference cycles

4. Security & Best Practices
- Check for XSS vulnerabilities
- Verify proper authentication and authorization
- Review form validation and sanitization
- Assess handling of sensitive data

5. Code Quality & Maintainability
- Improve component/function organization
- Enhance type safety and documentation
- Reduce complexity and improve readability
- Add proper error handling and logging

Provide your final updated code in triple backticks.
"""
    else:
        prompt = f"""
You are a highly experienced {project_type} developer and code reviewer.
Analyze the snippet from {fname}:


{code_input}


{similar_context}

Provide an in‚Äëdepth review across these sections, then output the updated code in triple backticks.

1. {project_type}-Specific Issues
{framework_guidance}

2. Syntax & Runtime Errors
- Identify syntax errors that would prevent compilation/execution
- Detect potential runtime exceptions and error handling issues
- Examine boundary conditions that could cause crashes

3. Design & Structure
- Evaluate adherence to {project_type} patterns and architectural principles
- Assess modularity, coupling, and cohesion metrics
- Review component/function responsibilities and organization

4. Performance & Scalability
- Identify inefficient rendering or data patterns
- Detect inefficient data structures or algorithms
- Review for optimization opportunities

5. Readability & Best Practices
- Check adherence to {project_type} style guides
- Evaluate naming conventions and consistency
- Assess code documentation and comments

6. Final Updated Code

Note : You will never change file names and path names unless you find a computational bug in it
"""

    # Add to chat history
    chat_entry = {
        "id": str(uuid.uuid4()),
        "timestamp": datetime.now().isoformat(),
        "type": "prompt",
        "content": prompt,
        "file": fname
    }
    chat_history.append(chat_entry)

    resp = model.generate_content(contents=[{"role": "user", "parts": [prompt]}])

    # Add to chat history
    response_entry = {
        "id": str(uuid.uuid4()),
        "timestamp": datetime.now().isoformat(),
        "type": "response",
        "content": resp.text,
        "file": fname
    }
    chat_history.append(response_entry)

    return resp.text

def extract_update(text):
    blocks = re.findall(r"(?:[a-zA-Z]*)?(?:\n|\r\n?)(.*?)", text, re.DOTALL)
    return blocks[-1].strip() if blocks else None

# 7Ô∏è‚É£ Process each code file with RAG context
def process_files(segregated, project_type, project_aim, embedding_manager):
    results = []

    for ext in sorted(os.listdir("code_dump")):
        folder = os.path.join("code_dump", ext)
        if not os.path.isdir(folder): continue

        for txt_file in sorted(os.listdir(folder)):
            fp = os.path.join(folder, txt_file)
            print(f"\nüìÇ Analyzing {fp} ‚Ä¶")

            try:
                with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
                    code = f.read()

                # Get similar files for context using RAG
                similar_files = None
                if embedding_manager.vectorstore:
                    query_result = embedding_manager.similarity_search(code, k=3)
                    similar_files = query_result if query_result else None

                # Run analysis with RAG context
                analysis = analyze_code(
                    code,
                    txt_file,
                    project_type,
                    aim=project_aim,
                    similar_files=similar_files
                )

                updated = extract_update(analysis)
                # Get embedding
                embedding = embedding_manager.get_document_embedding(code)[:10]
                # Check if it's a NumPy array or a list and handle accordingly
                if hasattr(embedding, 'tolist'):
                    embedding_list = embedding.tolist()
                else:
                    embedding_list = list(embedding)  # It's already a list, just make sure

                results.append({
                    "file": txt_file,
                    "lang": ext,
                    "analysis": analysis,
                    "updated": updated,
                    "embedding": embedding_list
                })


                # Overwrite the original extracted file if we have updated code
                if updated:
                    original_name = txt_file.rsplit('.',1)[0] + f".{ext}"
                    for root, _, fnames in os.walk("uploads"):
                        if original_name in fnames:
                            with open(os.path.join(root, original_name), 'w', encoding='utf-8') as f:
                                f.write(updated)

                print(f"‚úÖ Completed analysis of {txt_file}")
            except Exception as e:
                print(f"‚ö† Error processing {txt_file}: {str(e)}")

    return results
def add_embedding_visualization(embedding_manager, results):
    """Create visualization of document embeddings"""
    try:
        import matplotlib.pyplot as plt
        from sklearn.manifold import TSNE
        import numpy as np

        # Extract embeddings from results

        # Extract embeddings from results
        embeddings = []
        labels = []

        # Get embeddings for each file
        for result in results:
            with open(os.path.join("code_dump", result["lang"], result["file"]), 'r', encoding='utf-8', errors='ignore') as f:
                code = f.read()
                embedding = embedding_manager.get_document_embedding(code)
                # Ensure embedding is a NumPy array
                if not isinstance(embedding, np.ndarray):
                    embedding = np.array(embedding)
                embeddings.append(embedding)
                labels.append(f"{result['file']} ({result['lang']})")

        # Convert the list of embeddings to a NumPy array
        embeddings = np.array(embeddings)

        # Use t-SNE to reduce dimensions for visualization
        tsne = TSNE(n_components=2, random_state=42)
        reduced_embeddings = tsne.fit_transform(embeddings)


        # Create plot
        plt.figure(figsize=(12, 8))
        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)

        # Add labels for some points
        for i, (x, y) in enumerate(reduced_embeddings):
            if i % max(1, len(reduced_embeddings) // 10) == 0:  # Label every 10th point to avoid clutter
                plt.annotate(labels[i], (x, y), fontsize=8)

        plt.title("t-SNE Visualization of Code Embeddings")
        plt.savefig("embedding_visualization.png")
        print("‚úÖ Created embedding visualization")

        return "embedding_visualization.png"
    except Exception as e:
        print(f"‚ö† Error creating visualization: {str(e)}")
        return None


# 8Ô∏è‚É£ Create output ZIP
def create_output_zip(orig_zip):
    out_zip = orig_zip.rsplit('.',1)[0] + "_updated.zip"
    with zipfile.ZipFile(out_zip,'w',zipfile.ZIP_DEFLATED) as zout:
        for root, _, fnames in os.walk("uploads"):
            for fn in fnames:
                full = os.path.join(root, fn)
                arc = os.path.relpath(full, "uploads")
                zout.write(full, arc)
    print(f"\n‚úÖ Final ZIP created: {out_zip}")
    return out_zip

# 9Ô∏è‚É£ Save results and chat history
def save_results(results):
    with open("analysis_results.json", "w") as jf:
        json.dump(results, jf, indent=2)

    with open("chat_history.json", "w") as jf:
        json.dump(chat_history, jf, indent=2)

    print("‚úÖ Saved analysis results and chat history")

# üîü Set up a simple file server with ngrok
def start_file_server():
    # Create output directory for serving files
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)

    # Copy the updated ZIP and JSON files to the output directory
    if os.path.exists(out_zip):
        shutil.copy(out_zip, os.path.join(output_dir, os.path.basename(out_zip)))

    if os.path.exists("analysis_results.json"):
        shutil.copy("analysis_results.json", os.path.join(output_dir, "analysis_results.json"))

    if os.path.exists("chat_history.json"):
        shutil.copy("chat_history.json", os.path.join(output_dir, "chat_history.json"))

    # Create a simple index.html
    with open(os.path.join(output_dir, "index.html"), "w") as f:
        f.write(f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AI Software Engineer Results</title>
            <style>
                body {{ font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }}
                h1 {{ color: #333; }}
                .file-link {{ display: block; margin: 10px 0; padding: 10px; background: #f4f4f4; text-decoration: none; color: #333; border-radius: 4px; }}
                .file-link:hover {{ background: #e0e0e0; }}
            </style>
        </head>
        <body>
            <h1>AI Software Engineer Results</h1>
            <p>Project Type: {project_type}</p>
            <p>Files Analyzed: {len(results)}</p>
            <p>Last Updated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>

            <h2>Download Files</h2>
            <a class="file-link" href="{os.path.basename(out_zip)}">Download Updated ZIP</a>
            <a class="file-link" href="analysis_results.json">Download Analysis Results</a>
            <a class="file-link" href="chat_history.json">Download Chat History</a>
        </body>
        </html>
        """)

    # Change to the output directory
    os.chdir(output_dir)

    # Set up a simple HTTP server
    handler = http.server.SimpleHTTPRequestHandler
    httpd = socketserver.TCPServer(("", 8000), handler)

    # Start ngrok tunnel to port 8000
    public_url = ngrok.connect(8000)
    print(f"üåê Public URL: {public_url}")

    # Start HTTP server
    print("Serving files at port 8000")
    httpd.serve_forever()

# Main execution flow
# Main execution flow
if _name_ == "_main_":
    try:
        orig_zip = upload_and_extract()
        project_aim, project_type, config_files = analyze_project_structure("uploads")
        segregated, special_dirs = segregate_files("uploads")
        # human‚Äêin‚Äêthe‚Äêloop fallback
        if not project_aim:
            answer = input("üîç No README.md found. Would you like to provide a project description? (y/n): ")
            if answer.strip().lower() == 'y':
                project_aim = input("‚úç Please enter the project aim or overview:\n")
                if not project_aim.strip():
                    print("‚ö† Empty input received. Proceeding without a manual description.")
                    project_aim = None
        else:
            print("ü§ñ Proceeding with fully automated analysis.")

        # Use our enhanced vector embedding system
        vectorstore, embedding_manager = build_rag_knowledge_base(segregated, project_aim, config_files)

        # Add a semantic search capability to the project
        print("\nüîç Testing semantic search capability...")
        if project_aim:
            test_query = "main functionality"
            print(f"Query: '{test_query}'")
            search_results = embedding_manager.similarity_search(test_query, k=2)
            for doc, score in search_results:
                print(f"- {doc.metadata['source']} (score: {score:.4f})")
                print(f"  {doc.page_content[:100]}...\n")

        # Process files with our embedding manager
        results = process_files(segregated, project_type, project_aim, embedding_manager)

        # Save embedding model information
        with open("embedding_info.json", "w") as f:
            json.dump({
                "model": embedding_manager.embedding_type,
                "dimensions": embedding_manager.dimension,
                "files_processed": len(results)
            }, f, indent=2)

        # Create visualization
        vis_path = add_embedding_visualization(embedding_manager, results)

        out_zip = create_output_zip(orig_zip)
        save_results(results)

        # Start file server in a separate thread
        server_thread = threading.Thread(target=start_file_server, daemon=True)
        server_thread.start()

        print("\n" + "="*50)
        print("üéâ Everything is set up! Your files are being served through ngrok.")
        print("="*50)

        # Keep the main thread alive
        try:
            server_thread.join()
        except KeyboardInterrupt:
            print("Server stopped.")

    except Exception as e:
        print(f"‚ùå Error in main execution: {str(e)}")
