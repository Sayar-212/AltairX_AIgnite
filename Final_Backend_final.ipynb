{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhCvJZiy3Stt",
        "outputId": "89e713d3-5eeb-42fc-cb3d-a3ca9221f67d",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.31)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Install required packages\n",
        "!pip install -q google-generativeai langchain chromadb sentence-transformers pyngrok\n",
        "\n",
        "# 1️⃣ Imports & Config\n",
        "from google.colab import files as colab_files\n",
        "import google.generativeai as genai\n",
        "import zipfile, os, shutil, json, re\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "import http.server\n",
        "import socketserver\n",
        "import threading\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma, FAISS\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Configure Gemini API key (using your provided key)\n",
        "genai.configure(api_key=\"AIzaSyDDQfV8AolWWSIM6YatI-KVHpW9UaojXC0\")\n",
        "\n",
        "# Configure ngrok auth token (using your provided token)\n",
        "ngrok.set_auth_token(\"2vyG8rxMvWumV57l34zFYbJgXU7_5Eq6ntAuoqZvffhTQ9M2J\")\n",
        "\n",
        "# Expanded supported extensions\n",
        "EXT_DIRS = [\n",
        "    # Web & Frontend\n",
        "    'html','css','js','jsx','ts','tsx','vue','svelte','json','xml',\n",
        "    # Backend & Server\n",
        "    'py','php','rb','java','go','rs','cs','c','cpp',\n",
        "    # Config & Build\n",
        "    'yaml','yml','toml','ini','conf','env','lock','json','md','gitignore',\n",
        "    # Scripts & Others\n",
        "    'sh','bat','sql','pl','r','scala','bash','dockerfile'\n",
        "]\n",
        "\n",
        "# Frontend framework detection patterns\n",
        "FRAMEWORK_PATTERNS = {\n",
        "    'react': [\n",
        "        'import React', 'React.Component', 'useState', 'useEffect',\n",
        "        'ReactDOM', 'createRoot', 'jsx', 'tsx'\n",
        "    ],\n",
        "    'angular': [\n",
        "        '@Component', '@NgModule', '@Injectable', 'ngOnInit',\n",
        "        'ngAfterViewInit', 'ngFor', 'ngIf'\n",
        "    ],\n",
        "    'vue': [\n",
        "        'Vue.createApp', 'defineComponent', 'setup()', '<template>',\n",
        "        '@vue/cli', 'Composition API', 'Options API'\n",
        "    ],\n",
        "    'nodejs': [\n",
        "        'require(', 'module.exports', 'express()', 'npm', 'package.json',\n",
        "        'node_modules', 'process.env'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Initialize RAG components\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "vectorstore = None\n",
        "chat_history = []\n",
        "# Add this after the FRAMEWORK_PATTERNS dictionary\n",
        "class VectorEmbeddingManager:\n",
        "    def __init__(self, embedding_model=\"all-MiniLM-L6-v2\", embedding_type=\"huggingface\",\n",
        "                 persist_directory=\"./chroma_db\", openai_api_key=None):\n",
        "        self.embedding_type = embedding_type\n",
        "        self.persist_directory = persist_directory\n",
        "\n",
        "        # Initialize embeddings based on type\n",
        "        if embedding_type == \"huggingface\":\n",
        "            self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "            self.dimension = 384  # Default for all-MiniLM-L6-v2\n",
        "        elif embedding_type == \"openai\":\n",
        "            if not openai_api_key:\n",
        "                raise ValueError(\"OpenAI API key is required for OpenAI embeddings\")\n",
        "            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "            self.embeddings = OpenAIEmbeddings()\n",
        "            self.dimension = 1536  # Default for OpenAI embeddings\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported embedding type: {embedding_type}\")\n",
        "\n",
        "        self.vectorstore = None\n",
        "\n",
        "    def create_vectorstore(self, documents, vectorstore_type=\"chroma\"):\n",
        "        \"\"\"Create a vector store from documents\"\"\"\n",
        "        if vectorstore_type == \"chroma\":\n",
        "            self.vectorstore = Chroma.from_documents(\n",
        "                documents=documents,\n",
        "                embedding=self.embeddings,\n",
        "                persist_directory=self.persist_directory\n",
        "            )\n",
        "        elif vectorstore_type == \"faiss\":\n",
        "            self.vectorstore = FAISS.from_documents(\n",
        "                documents=documents,\n",
        "                embedding=self.embeddings\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported vectorstore type: {vectorstore_type}\")\n",
        "\n",
        "        return self.vectorstore\n",
        "\n",
        "    def add_documents(self, documents):\n",
        "        \"\"\"Add documents to existing vectorstore\"\"\"\n",
        "        if not self.vectorstore:\n",
        "            raise ValueError(\"Vectorstore not initialized. Call create_vectorstore first.\")\n",
        "\n",
        "        self.vectorstore.add_documents(documents)\n",
        "        return self.vectorstore\n",
        "\n",
        "    def similarity_search(self, query, k=5):\n",
        "        \"\"\"Perform similarity search\"\"\"\n",
        "        if not self.vectorstore:\n",
        "            raise ValueError(\"Vectorstore not initialized\")\n",
        "\n",
        "        return self.vectorstore.similarity_search_with_score(query, k=k)\n",
        "\n",
        "    def get_document_embedding(self, text):\n",
        "        \"\"\"Get embedding for a single text\"\"\"\n",
        "        return self.embeddings.embed_query(text)\n",
        "\n",
        "    def compute_similarity(self, text1, text2):\n",
        "        \"\"\"Compute cosine similarity between two texts\"\"\"\n",
        "        embedding1 = self.get_document_embedding(text1)\n",
        "        embedding2 = self.get_document_embedding(text2)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
        "        return similarity\n",
        "\n",
        "# 2️⃣ Upload & extract your ZIP\n",
        "def upload_and_extract():\n",
        "    uploaded = colab_files.upload()\n",
        "    orig_zip = next(iter(uploaded))\n",
        "    print(f\"📦 Uploaded: {orig_zip}\")\n",
        "\n",
        "    extract_root = \"uploads\"\n",
        "    if os.path.exists(extract_root): shutil.rmtree(extract_root)\n",
        "    os.makedirs(extract_root, exist_ok=True)\n",
        "    with zipfile.ZipFile(orig_zip, 'r') as z: z.extractall(extract_root)\n",
        "    print(\"✅ ZIP extracted to /content/uploads\")\n",
        "    return orig_zip\n",
        "\n",
        "# 3️⃣ Detect Project Structure and Config Files\n",
        "def analyze_project_structure(extract_root):\n",
        "    project_aim = None\n",
        "    project_type = \"unknown\"\n",
        "    config_files = {}\n",
        "    framework_scores = {framework: 0 for framework in FRAMEWORK_PATTERNS.keys()}\n",
        "\n",
        "    # Look for README and config files\n",
        "    for root, _, files in os.walk(extract_root):\n",
        "        for file in files:\n",
        "            file_lower = file.lower()\n",
        "            filepath = os.path.join(root, file)\n",
        "\n",
        "            # Find README\n",
        "            if file_lower == \"readme.md\":\n",
        "                with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    project_aim = f.read()\n",
        "                print(\"🧠 README.md found — using it as the project aim.\")\n",
        "\n",
        "            # Collect config files\n",
        "            if file_lower in [\"package.json\", \"angular.json\", \"tsconfig.json\",\n",
        "                             \"webpack.config.js\", \".babelrc\", \"vite.config.js\",\n",
        "                             \"nuxt.config.js\", \"next.config.js\", \"svelte.config.js\"]:\n",
        "                try:\n",
        "                    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                        config_files[file_lower] = f.read()\n",
        "                    print(f\"📄 Found config file: {file_lower}\")\n",
        "                except:\n",
        "                    print(f\"⚠️ Error reading config file: {file_lower}\")\n",
        "\n",
        "            # Analyze file content to detect frameworks\n",
        "            try:\n",
        "                _, ext = os.path.splitext(file_lower)\n",
        "                if ext[1:] in ['js', 'jsx', 'ts', 'tsx', 'html', 'vue', 'svelte']:\n",
        "                    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                        content = f.read()\n",
        "                        for framework, patterns in FRAMEWORK_PATTERNS.items():\n",
        "                            for pattern in patterns:\n",
        "                                if pattern in content:\n",
        "                                    framework_scores[framework] += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # Determine project type from scores and config files\n",
        "    if \"package.json\" in config_files:\n",
        "        project_type = \"nodejs\"\n",
        "        try:\n",
        "            package_json = json.loads(config_files[\"package.json\"])\n",
        "            dependencies = {**package_json.get(\"dependencies\", {}), **package_json.get(\"devDependencies\", {})}\n",
        "\n",
        "            if \"react\" in dependencies or \"react-dom\" in dependencies:\n",
        "                project_type = \"react\"\n",
        "                if \"next\" in dependencies:\n",
        "                    project_type = \"react-nextjs\"\n",
        "            elif \"vue\" in dependencies:\n",
        "                project_type = \"vue\"\n",
        "                if \"nuxt\" in dependencies:\n",
        "                    project_type = \"vue-nuxt\"\n",
        "            elif \"@angular/core\" in dependencies:\n",
        "                project_type = \"angular\"\n",
        "            elif \"svelte\" in dependencies:\n",
        "                project_type = \"svelte\"\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # If config detection didn't work, use pattern matching results\n",
        "    if project_type == \"nodejs\":\n",
        "        max_framework = max(framework_scores.items(), key=lambda x: x[1])\n",
        "        if max_framework[1] > 0:\n",
        "            project_type = max_framework[0]\n",
        "\n",
        "    print(f\"🔍 Detected project type: {project_type}\")\n",
        "    return project_aim, project_type, config_files\n",
        "\n",
        "# 4️⃣ Segregate code files with improved classification\n",
        "def segregate_files(extract_root):\n",
        "    segregated = {ext: [] for ext in EXT_DIRS}\n",
        "    special_dirs = {\n",
        "        \"components\": [],\n",
        "        \"pages\": [],\n",
        "        \"hooks\": [],\n",
        "        \"services\": [],\n",
        "        \"utils\": [],\n",
        "        \"assets\": [],\n",
        "        \"styles\": [],\n",
        "        \"api\": [],\n",
        "        \"tests\": []\n",
        "    }\n",
        "\n",
        "    for root, dirs, fnames in os.walk(extract_root):\n",
        "        # Skip node_modules and other large dependency directories\n",
        "        if any(excluded in root for excluded in [\"node_modules\", \"dist\", \"build\", \".git\"]):\n",
        "            continue\n",
        "\n",
        "        # Check if we're in a special directory\n",
        "        rel_path = os.path.relpath(root, extract_root)\n",
        "        current_dir = os.path.basename(root).lower()\n",
        "        special_dir_match = None\n",
        "        for special_dir in special_dirs.keys():\n",
        "            if special_dir in current_dir or special_dir in rel_path.lower():\n",
        "                special_dir_match = special_dir\n",
        "                break\n",
        "\n",
        "        for fn in fnames:\n",
        "            if '.' not in fn: continue\n",
        "            ext = fn.rsplit('.',1)[1].lower()\n",
        "            if ext in EXT_DIRS:\n",
        "                tgt = os.path.join(\"code_dump\", ext)\n",
        "                os.makedirs(tgt, exist_ok=True)\n",
        "                src = os.path.join(root, fn)\n",
        "                dst = os.path.join(tgt, fn)\n",
        "                shutil.copy(src, dst)\n",
        "                segregated[ext].append(dst)\n",
        "\n",
        "                # Also add to special directory if applicable\n",
        "                if special_dir_match:\n",
        "                    tgt_special = os.path.join(\"code_dump_organized\", special_dir_match)\n",
        "                    os.makedirs(tgt_special, exist_ok=True)\n",
        "                    dst_special = os.path.join(tgt_special, fn)\n",
        "                    shutil.copy(src, dst_special)\n",
        "                    special_dirs[special_dir_match].append(dst_special)\n",
        "\n",
        "    print(\"✅ Files segregated into code_dump/ and code_dump_organized/\")\n",
        "    return segregated, special_dirs\n",
        "\n",
        "# 5️⃣ Create RAG knowledge base from code\n",
        "def build_rag_knowledge_base(segregated, project_aim, config_files, embedding_model=\"all-MiniLM-L6-v2\"):\n",
        "    global vectorstore\n",
        "\n",
        "    # Initialize the embedding manager\n",
        "    embedding_manager = VectorEmbeddingManager(\n",
        "        embedding_model=embedding_model,\n",
        "        embedding_type=\"huggingface\",\n",
        "        persist_directory=\"./chroma_db\"\n",
        "    )\n",
        "\n",
        "    # Prepare documents for RAG\n",
        "    documents = []\n",
        "\n",
        "    # Add project aim\n",
        "    if project_aim:\n",
        "        documents.append({\n",
        "            \"content\": project_aim,\n",
        "            \"metadata\": {\"source\": \"README.md\", \"type\": \"documentation\"}\n",
        "        })\n",
        "\n",
        "    # Add config files\n",
        "    for config_name, content in config_files.items():\n",
        "        documents.append({\n",
        "            \"content\": content,\n",
        "            \"metadata\": {\"source\": config_name, \"type\": \"configuration\"}\n",
        "        })\n",
        "\n",
        "    # Add code files\n",
        "    for ext, paths in segregated.items():\n",
        "        for path in paths:\n",
        "            try:\n",
        "                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    content = f.read()\n",
        "                    filename = os.path.basename(path)\n",
        "                    documents.append({\n",
        "                        \"content\": content,\n",
        "                        \"metadata\": {\"source\": filename, \"language\": ext, \"type\": \"code\"}\n",
        "                    })\n",
        "            except:\n",
        "                print(f\"⚠️ Error reading file: {path}\")\n",
        "\n",
        "    # Create text chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        doc_chunks = text_splitter.create_documents(\n",
        "            texts=[doc[\"content\"]],\n",
        "            metadatas=[doc[\"metadata\"]]\n",
        "        )\n",
        "        chunks.extend(doc_chunks)\n",
        "\n",
        "    # Create vector store\n",
        "    vectorstore = embedding_manager.create_vectorstore(chunks)\n",
        "\n",
        "    print(f\"✅ Created RAG knowledge base with {len(chunks)} chunks from {len(documents)} documents\")\n",
        "    return vectorstore, embedding_manager\n",
        "\n",
        "# 6️⃣ Gemini Analysis Function with Framework-Specific Prompts\n",
        "def analyze_code(code_input, fname, project_type, aim=None, similar_files=None):\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")\n",
        "\n",
        "    # Create framework-specific guidance based on detected project type\n",
        "    framework_guidance = \"\"\n",
        "    if project_type == \"react\":\n",
        "        framework_guidance = \"\"\"\n",
        "- Check for React best practices (hooks rules, useCallback/useMemo optimization)\n",
        "- Ensure components follow Single Responsibility Principle\n",
        "- Verify proper state management (useState, useReducer, Context, or external state)\n",
        "- Check for correct key usage in lists\n",
        "- Ensure proper effect cleanup to prevent memory leaks\n",
        "- Assess component re-rendering optimization\n",
        "\"\"\"\n",
        "    elif project_type == \"angular\":\n",
        "        framework_guidance = \"\"\"\n",
        "- Check for proper component lifecycle management\n",
        "- Verify correct usage of services and dependency injection\n",
        "- Assess template syntax and binding correctness\n",
        "- Evaluate usage of Angular directives and pipes\n",
        "- Check for proper module organization\n",
        "- Verify correct use of observables and subscription management\n",
        "\"\"\"\n",
        "    elif project_type == \"vue\":\n",
        "        framework_guidance = \"\"\"\n",
        "- Verify proper component lifecycle hook usage\n",
        "- Check correct usage of computed properties vs methods\n",
        "- Evaluate props validation and component communication\n",
        "- Assess reactivity edge cases and limitations\n",
        "- Verify proper usage of Vue-specific directives\n",
        "- Check correct store pattern implementation (Vuex/Pinia)\n",
        "\"\"\"\n",
        "    elif project_type == \"nodejs\":\n",
        "        framework_guidance = \"\"\"\n",
        "- Check for proper async/await and Promise handling\n",
        "- Verify error handling and middleware patterns\n",
        "- Assess database connection management\n",
        "- Check for security best practices (input validation, auth)\n",
        "- Evaluate API design and RESTful principles\n",
        "- Verify environment variable usage and configuration\n",
        "\"\"\"\n",
        "\n",
        "    # Get similar files context if available\n",
        "    similar_context = \"\"\n",
        "    if similar_files:\n",
        "        similar_context = \"Similar files in the project that might provide context:\\n\"\n",
        "        for i, (file, score) in enumerate(similar_files[:3]):\n",
        "            similar_context += f\"{i+1}. {file.metadata['source']} (similarity: {score:.2f})\\n\"\n",
        "\n",
        "    if aim:\n",
        "        prompt = f\"\"\"\n",
        "You are a professional software architect specifically experienced with {project_type} development.\n",
        "The project goal is:\n",
        "\n",
        "📘 **Project Aim (from README.md)**:\n",
        "{aim.strip()}\n",
        "\n",
        "{similar_context}\n",
        "\n",
        "Now analyze the following file `{fname}` and refactor it while preserving project intent:\n",
        "\n",
        "```\n",
        "{code_input}\n",
        "```\n",
        "\n",
        "Provide an in-depth review and updated version of the file focused on {project_type} best practices:\n",
        "\n",
        "1. Framework-Specific Issues\n",
        "{framework_guidance}\n",
        "\n",
        "2. Logical Errors & Edge Cases\n",
        "- Analyze conditional logic for correctness and completeness\n",
        "- Identify off-by-one errors and boundary condition handling\n",
        "- Verify business logic implementation\n",
        "- Check for race conditions in async code\n",
        "\n",
        "3. Performance & Scalability\n",
        "- Optimize rendering performance (for UI components)\n",
        "- Improve data fetching and caching strategies\n",
        "- Enhance bundle size optimization techniques\n",
        "- Identify memory leaks from closures or reference cycles\n",
        "\n",
        "4. Security & Best Practices\n",
        "- Check for XSS vulnerabilities\n",
        "- Verify proper authentication and authorization\n",
        "- Review form validation and sanitization\n",
        "- Assess handling of sensitive data\n",
        "\n",
        "5. Code Quality & Maintainability\n",
        "- Improve component/function organization\n",
        "- Enhance type safety and documentation\n",
        "- Reduce complexity and improve readability\n",
        "- Add proper error handling and logging\n",
        "\n",
        "Provide your final updated code in triple backticks.\n",
        "\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"\n",
        "You are a highly experienced {project_type} developer and code reviewer.\n",
        "Analyze the snippet from `{fname}`:\n",
        "\n",
        "```\n",
        "{code_input}\n",
        "```\n",
        "\n",
        "{similar_context}\n",
        "\n",
        "Provide an in‑depth review across these sections, then output the updated code in triple backticks.\n",
        "\n",
        "1. {project_type}-Specific Issues\n",
        "{framework_guidance}\n",
        "\n",
        "2. Syntax & Runtime Errors\n",
        "- Identify syntax errors that would prevent compilation/execution\n",
        "- Detect potential runtime exceptions and error handling issues\n",
        "- Examine boundary conditions that could cause crashes\n",
        "\n",
        "3. Design & Structure\n",
        "- Evaluate adherence to {project_type} patterns and architectural principles\n",
        "- Assess modularity, coupling, and cohesion metrics\n",
        "- Review component/function responsibilities and organization\n",
        "\n",
        "4. Performance & Scalability\n",
        "- Identify inefficient rendering or data patterns\n",
        "- Detect inefficient data structures or algorithms\n",
        "- Review for optimization opportunities\n",
        "\n",
        "5. Readability & Best Practices\n",
        "- Check adherence to {project_type} style guides\n",
        "- Evaluate naming conventions and consistency\n",
        "- Assess code documentation and comments\n",
        "\n",
        "6. Final Updated Code\n",
        "\n",
        "Note : You will never change file names and path names unless you find a computational bug in it\n",
        "\"\"\"\n",
        "\n",
        "    # Add to chat history\n",
        "    chat_entry = {\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"type\": \"prompt\",\n",
        "        \"content\": prompt,\n",
        "        \"file\": fname\n",
        "    }\n",
        "    chat_history.append(chat_entry)\n",
        "\n",
        "    resp = model.generate_content(contents=[{\"role\": \"user\", \"parts\": [prompt]}])\n",
        "\n",
        "    # Add to chat history\n",
        "    response_entry = {\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"type\": \"response\",\n",
        "        \"content\": resp.text,\n",
        "        \"file\": fname\n",
        "    }\n",
        "    chat_history.append(response_entry)\n",
        "\n",
        "    return resp.text\n",
        "\n",
        "def extract_update(text):\n",
        "    blocks = re.findall(r\"```(?:[a-zA-Z]*)?(?:\\n|\\r\\n?)(.*?)```\", text, re.DOTALL)\n",
        "    return blocks[-1].strip() if blocks else None\n",
        "\n",
        "# 7️⃣ Process each code file with RAG context\n",
        "def process_files(segregated, project_type, project_aim, embedding_manager):\n",
        "    results = []\n",
        "\n",
        "    for ext in sorted(os.listdir(\"code_dump\")):\n",
        "        folder = os.path.join(\"code_dump\", ext)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        for txt_file in sorted(os.listdir(folder)):\n",
        "            fp = os.path.join(folder, txt_file)\n",
        "            print(f\"\\n📂 Analyzing {fp} …\")\n",
        "\n",
        "            try:\n",
        "                with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                    code = f.read()\n",
        "\n",
        "                # Get similar files for context using RAG\n",
        "                similar_files = None\n",
        "                if embedding_manager.vectorstore:\n",
        "                    query_result = embedding_manager.similarity_search(code, k=3)\n",
        "                    similar_files = query_result if query_result else None\n",
        "\n",
        "                # Run analysis with RAG context\n",
        "                analysis = analyze_code(\n",
        "                    code,\n",
        "                    txt_file,\n",
        "                    project_type,\n",
        "                    aim=project_aim,\n",
        "                    similar_files=similar_files\n",
        "                )\n",
        "\n",
        "                updated = extract_update(analysis)\n",
        "                # Get embedding\n",
        "                embedding = embedding_manager.get_document_embedding(code)[:10]\n",
        "                # Check if it's a NumPy array or a list and handle accordingly\n",
        "                if hasattr(embedding, 'tolist'):\n",
        "                    embedding_list = embedding.tolist()\n",
        "                else:\n",
        "                    embedding_list = list(embedding)  # It's already a list, just make sure\n",
        "\n",
        "                results.append({\n",
        "                    \"file\": txt_file,\n",
        "                    \"lang\": ext,\n",
        "                    \"analysis\": analysis,\n",
        "                    \"updated\": updated,\n",
        "                    \"embedding\": embedding_list\n",
        "                })\n",
        "\n",
        "\n",
        "                # Overwrite the original extracted file if we have updated code\n",
        "                if updated:\n",
        "                    original_name = txt_file.rsplit('.',1)[0] + f\".{ext}\"\n",
        "                    for root, _, fnames in os.walk(\"uploads\"):\n",
        "                        if original_name in fnames:\n",
        "                            with open(os.path.join(root, original_name), 'w', encoding='utf-8') as f:\n",
        "                                f.write(updated)\n",
        "\n",
        "                print(f\"✅ Completed analysis of {txt_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error processing {txt_file}: {str(e)}\")\n",
        "\n",
        "    return results\n",
        "def add_embedding_visualization(embedding_manager, results):\n",
        "    \"\"\"Create visualization of document embeddings\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        from sklearn.manifold import TSNE\n",
        "        import numpy as np\n",
        "\n",
        "        # Extract embeddings from results\n",
        "\n",
        "        # Extract embeddings from results\n",
        "        embeddings = []\n",
        "        labels = []\n",
        "\n",
        "        # Get embeddings for each file\n",
        "        for result in results:\n",
        "            with open(os.path.join(\"code_dump\", result[\"lang\"], result[\"file\"]), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                code = f.read()\n",
        "                embedding = embedding_manager.get_document_embedding(code)\n",
        "                # Ensure embedding is a NumPy array\n",
        "                if not isinstance(embedding, np.ndarray):\n",
        "                    embedding = np.array(embedding)\n",
        "                embeddings.append(embedding)\n",
        "                labels.append(f\"{result['file']} ({result['lang']})\")\n",
        "\n",
        "        # Convert the list of embeddings to a NumPy array\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        # Use t-SNE to reduce dimensions for visualization\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
        "\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n",
        "\n",
        "        # Add labels for some points\n",
        "        for i, (x, y) in enumerate(reduced_embeddings):\n",
        "            if i % max(1, len(reduced_embeddings) // 10) == 0:  # Label every 10th point to avoid clutter\n",
        "                plt.annotate(labels[i], (x, y), fontsize=8)\n",
        "\n",
        "        plt.title(\"t-SNE Visualization of Code Embeddings\")\n",
        "        plt.savefig(\"embedding_visualization.png\")\n",
        "        print(\"✅ Created embedding visualization\")\n",
        "\n",
        "        return \"embedding_visualization.png\"\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error creating visualization: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# 8️⃣ Create output ZIP\n",
        "def create_output_zip(orig_zip):\n",
        "    out_zip = orig_zip.rsplit('.',1)[0] + \"_updated.zip\"\n",
        "    with zipfile.ZipFile(out_zip,'w',zipfile.ZIP_DEFLATED) as zout:\n",
        "        for root, _, fnames in os.walk(\"uploads\"):\n",
        "            for fn in fnames:\n",
        "                full = os.path.join(root, fn)\n",
        "                arc = os.path.relpath(full, \"uploads\")\n",
        "                zout.write(full, arc)\n",
        "    print(f\"\\n✅ Final ZIP created: {out_zip}\")\n",
        "    return out_zip\n",
        "\n",
        "# 9️⃣ Save results and chat history\n",
        "def save_results(results):\n",
        "    with open(\"analysis_results.json\", \"w\") as jf:\n",
        "        json.dump(results, jf, indent=2)\n",
        "\n",
        "    with open(\"chat_history.json\", \"w\") as jf:\n",
        "        json.dump(chat_history, jf, indent=2)\n",
        "\n",
        "    print(\"✅ Saved analysis results and chat history\")\n",
        "\n",
        "# 🔟 Set up a simple file server with ngrok\n",
        "def start_file_server():\n",
        "    # Create output directory for serving files\n",
        "    output_dir = \"output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Copy the updated ZIP and JSON files to the output directory\n",
        "    if os.path.exists(out_zip):\n",
        "        shutil.copy(out_zip, os.path.join(output_dir, os.path.basename(out_zip)))\n",
        "\n",
        "    if os.path.exists(\"analysis_results.json\"):\n",
        "        shutil.copy(\"analysis_results.json\", os.path.join(output_dir, \"analysis_results.json\"))\n",
        "\n",
        "    if os.path.exists(\"chat_history.json\"):\n",
        "        shutil.copy(\"chat_history.json\", os.path.join(output_dir, \"chat_history.json\"))\n",
        "\n",
        "    # Create a simple index.html\n",
        "    with open(os.path.join(output_dir, \"index.html\"), \"w\") as f:\n",
        "        f.write(f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>AI Software Engineer Results</title>\n",
        "            <style>\n",
        "                body {{ font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }}\n",
        "                h1 {{ color: #333; }}\n",
        "                .file-link {{ display: block; margin: 10px 0; padding: 10px; background: #f4f4f4; text-decoration: none; color: #333; border-radius: 4px; }}\n",
        "                .file-link:hover {{ background: #e0e0e0; }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h1>AI Software Engineer Results</h1>\n",
        "            <p>Project Type: {project_type}</p>\n",
        "            <p>Files Analyzed: {len(results)}</p>\n",
        "            <p>Last Updated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
        "\n",
        "            <h2>Download Files</h2>\n",
        "            <a class=\"file-link\" href=\"{os.path.basename(out_zip)}\">Download Updated ZIP</a>\n",
        "            <a class=\"file-link\" href=\"analysis_results.json\">Download Analysis Results</a>\n",
        "            <a class=\"file-link\" href=\"chat_history.json\">Download Chat History</a>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\")\n",
        "\n",
        "    # Change to the output directory\n",
        "    os.chdir(output_dir)\n",
        "\n",
        "    # Set up a simple HTTP server\n",
        "    handler = http.server.SimpleHTTPRequestHandler\n",
        "    httpd = socketserver.TCPServer((\"\", 8000), handler)\n",
        "\n",
        "    # Start ngrok tunnel to port 8000\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"🌐 Public URL: {public_url}\")\n",
        "\n",
        "    # Start HTTP server\n",
        "    print(\"Serving files at port 8000\")\n",
        "    httpd.serve_forever()\n",
        "\n",
        "# Main execution flow\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        orig_zip = upload_and_extract()\n",
        "        project_aim, project_type, config_files = analyze_project_structure(\"uploads\")\n",
        "        segregated, special_dirs = segregate_files(\"uploads\")\n",
        "\n",
        "        # Use our enhanced vector embedding system\n",
        "        vectorstore, embedding_manager = build_rag_knowledge_base(segregated, project_aim, config_files)\n",
        "\n",
        "        # Add a semantic search capability to the project\n",
        "        print(\"\\n🔍 Testing semantic search capability...\")\n",
        "        if project_aim:\n",
        "            test_query = \"main functionality\"\n",
        "            print(f\"Query: '{test_query}'\")\n",
        "            search_results = embedding_manager.similarity_search(test_query, k=2)\n",
        "            for doc, score in search_results:\n",
        "                print(f\"- {doc.metadata['source']} (score: {score:.4f})\")\n",
        "                print(f\"  {doc.page_content[:100]}...\\n\")\n",
        "\n",
        "        # Process files with our embedding manager\n",
        "        results = process_files(segregated, project_type, project_aim, embedding_manager)\n",
        "\n",
        "        # Save embedding model information\n",
        "        with open(\"embedding_info.json\", \"w\") as f:\n",
        "            json.dump({\n",
        "                \"model\": embedding_manager.embedding_type,\n",
        "                \"dimensions\": embedding_manager.dimension,\n",
        "                \"files_processed\": len(results)\n",
        "            }, f, indent=2)\n",
        "\n",
        "        # Create visualization\n",
        "        vis_path = add_embedding_visualization(embedding_manager, results)\n",
        "\n",
        "        out_zip = create_output_zip(orig_zip)\n",
        "        save_results(results)\n",
        "\n",
        "        # Start file server in a separate thread\n",
        "        server_thread = threading.Thread(target=start_file_server, daemon=True)\n",
        "        server_thread.start()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"🎉 Everything is set up! Your files are being served through ngrok.\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Keep the main thread alive\n",
        "        try:\n",
        "            server_thread.join()\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Server stopped.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in main execution: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "QUKUbx7YqJgo",
        "outputId": "daedacbd-7e7c-411e-9a33-e4461f0be12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-85cd4aa3f488>:62: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ca944c85-a59a-4542-8329-bba159f98e3f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ca944c85-a59a-4542-8329-bba159f98e3f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Altair-main.zip to Altair-main (1).zip\n",
            "📦 Uploaded: Altair-main (1).zip\n",
            "✅ ZIP extracted to /content/uploads\n",
            "🔍 Detected project type: unknown\n",
            "✅ Files segregated into code_dump/ and code_dump_organized/\n",
            "✅ Created RAG knowledge base with 55 chunks from 3 documents\n",
            "\n",
            "🔍 Testing semantic search capability...\n",
            "\n",
            "📂 Analyzing code_dump/html/altair-login-page.html …\n",
            "✅ Completed analysis of altair-login-page.html\n",
            "\n",
            "📂 Analyzing code_dump/html/index.html …\n",
            "✅ Completed analysis of index.html\n",
            "\n",
            "📂 Analyzing code_dump/html/mainf.html …\n",
            "✅ Completed analysis of mainf.html\n",
            "⚠️ Error creating visualization: perplexity must be less than n_samples\n",
            "\n",
            "✅ Final ZIP created: Altair-main (1)_updated.zip\n",
            "✅ Saved analysis results and chat history\n",
            "\n",
            "==================================================\n",
            "🎉 Everything is set up! Your files are being served through ngrok.\n",
            "==================================================\n",
            "🌐 Public URL: NgrokTunnel: \"https://c3f2-34-142-142-176.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "Serving files at port 8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [20/Apr/2025 01:13:35] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [20/Apr/2025 01:13:36] code 404, message File not found\n",
            "127.0.0.1 - - [20/Apr/2025 01:13:36] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
            "127.0.0.1 - - [20/Apr/2025 01:13:43] \"GET /Altair-main%20(1)_updated.zip HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ]
}